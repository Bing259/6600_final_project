{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b33603c",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46b3244c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" : jewish harvard professor noel ignatiev w...</td>\n",
       "      <td>implicit_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b.higher education is a part of european cult...</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has a problem with  \" the whites \" \" and \" \" ...</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is yasir qadhi a hate preacher for calling ch...</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt  \" : how three million germans mass murder...</td>\n",
       "      <td>not_hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post          class\n",
       "0    \" : jewish harvard professor noel ignatiev w...  implicit_hate\n",
       "1   b.higher education is a part of european cult...       not_hate\n",
       "2   has a problem with  \" the whites \" \" and \" \" ...       not_hate\n",
       "3   is yasir qadhi a hate preacher for calling ch...       not_hate\n",
       "4   rt  \" : how three million germans mass murder...       not_hate"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "# 1. Load the original TSV\n",
    "df = pd.read_csv('./data/raw/hate_speech_dataset.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e104082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_post</th>\n",
       "      <th>class</th>\n",
       "      <th>bi_class</th>\n",
       "      <th>mul_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jewish harvard professor noel ignatiev wants a...</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>higher education european culture imported con...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>problem whites christians ahead free say</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yasir qadhi hate preacher calling christians f...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>million germans mass murdered destruction reich</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_post          class  bi_class  \\\n",
       "0  jewish harvard professor noel ignatiev wants a...  implicit_hate         1   \n",
       "1  higher education european culture imported con...       not_hate         0   \n",
       "2           problem whites christians ahead free say       not_hate         0   \n",
       "3  yasir qadhi hate preacher calling christians f...       not_hate         0   \n",
       "4    million germans mass murdered destruction reich       not_hate         0   \n",
       "\n",
       "   mul_class  \n",
       "0          1  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a text cleaning function\n",
    "stopwords = ENGLISH_STOP_WORDS\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove \"rt\" as retweet marker\n",
    "    text = re.sub(r\"\\brt\\b\", \" \", text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r\"[@#]\\w+\", \" \", text)\n",
    "    # Remove HTML entities\n",
    "    text = re.sub(r\"&\\w+;\", \" \", text)\n",
    "    # Remove non-letter characters, keep spaces and apostrophes\n",
    "    text = re.sub(r\"[^a-z\\s']\", \" \", text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Tokenize on spaces\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords and very short tokens\n",
    "    tokens = [t for t in tokens if t not in stopwords and len(t) > 1]\n",
    "    # Spell correction for each token\n",
    "    # corrected_tokens = [spell.correction(t) or t for t in tokens]\n",
    "    # Join back to string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['cleaned_post'] = df['post'].apply(clean_text)\n",
    "\n",
    "# Drop empty rows (only 30 rows)\n",
    "df = df[df[\"cleaned_post\"] != \"\"]\n",
    "\n",
    "# Map class labels to numeric codes\n",
    "mul_class_mapping = {\n",
    "    'not_hate': 0,\n",
    "    'implicit_hate': 1,\n",
    "    'explicit_hate': 2\n",
    "}\n",
    "bi_class_mapping = {\n",
    "    'not_hate': 0,\n",
    "    'implicit_hate': 1,\n",
    "    'explicit_hate': 1\n",
    "}\n",
    "df['bi_class'] = df['class'].map(bi_class_mapping)\n",
    "df['mul_class'] = df['class'].map(mul_class_mapping)\n",
    "\n",
    "# Keep only needed columns\n",
    "cleaned_df = df[['cleaned_post', 'class', 'bi_class', 'mul_class']]\n",
    "\n",
    "# Save the cleaned data\n",
    "output_dir = './data/cleaned'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, 'hate_speech_dataset.tsv')\n",
    "cleaned_df.to_csv(output_path, index=False)\n",
    "\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e417e59",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639a33b",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5cfd5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set TF-IDF shape: (17160, 5000)\n",
      "Test set TF-IDF shape: (4290, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned data\n",
    "df = pd.read_csv(\"./data/cleaned/hate_speech_dataset.tsv\")\n",
    "\n",
    "# Text and labels\n",
    "X_text = df[\"cleaned_post\"].astype(str)\n",
    "y = df[\"bi_class\"].values   # Binary class labels 0/1\n",
    "\n",
    "# Split train and test sets (for later model training)\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,   # Stratified sampling by class\n",
    ")\n",
    "\n",
    "# Define TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000, \n",
    "    ngram_range=(1, 2),     # Use unigram + bigram\n",
    "    min_df=5,               # Keep terms that appear in at least 5 documents\n",
    ")\n",
    "\n",
    "# 4. Fit on training set, then transform training and test sets\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "\n",
    "print(\"Training set TF-IDF shape:\", X_train_tfidf.shape)\n",
    "print(\"Test set TF-IDF shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705466b8",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3dc1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set embedding shape: (17160, 100)\n",
      "Testing set embedding shape: (4290, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the cleaned data\n",
    "df = pd.read_csv(\"./data/cleaned/hate_speech_dataset.tsv\")\n",
    "\n",
    "y_bi = df[\"bi_class\"].values     # Binary class 0 1\n",
    "\n",
    "# Split train/test for later modeling\n",
    "X_text = df[\"cleaned_post\"].astype(str)\n",
    "\n",
    "X_train_text, X_test_text, y_train_bi, y_test_bi = train_test_split(\n",
    "    X_text,\n",
    "    y_bi,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_bi,\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "sentences = [word_tokenize(row) for row in X_train_text]\n",
    "\n",
    "# Train Word2Vec model\n",
    "emb_dim = 100\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=emb_dim,\n",
    "    window=5,\n",
    "    min_count=5,   # At least appear in 5 sentences\n",
    "    workers=4,\n",
    "    sg=1           # 1 for skip-gram, 0 for CBOW\n",
    ")\n",
    "\n",
    "# Convert a sentence into an average word vector\n",
    "def sentence_to_vec(sentence: str, model: Word2Vec, emb_dim: int) -> np.ndarray:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    vecs = []\n",
    "    for w in tokens:\n",
    "        if w in model.wv:\n",
    "            vecs.append(model.wv[w])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(emb_dim)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# Construct embedding feature matrices\n",
    "X_train_emb = np.vstack([\n",
    "    sentence_to_vec(s, w2v_model, emb_dim)\n",
    "    for s in X_train_text\n",
    "])\n",
    "\n",
    "X_test_emb = np.vstack([\n",
    "    sentence_to_vec(s, w2v_model, emb_dim)\n",
    "    for s in X_test_text\n",
    "])\n",
    "\n",
    "print(\"Training set embedding shape:\", X_train_emb.shape)\n",
    "print(\"Testing set embedding shape:\", X_test_emb.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
